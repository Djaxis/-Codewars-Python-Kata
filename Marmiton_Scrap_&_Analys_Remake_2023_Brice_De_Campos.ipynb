{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnaSM5gkt3zfK4U/FQ7/dD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Djaxis/MY-Python-Evolution/blob/main/Marmiton_Scrap_%26_Analys_Remake_2023_Brice_De_Campos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUIbSwUeFMu8"
      },
      "outputs": [],
      "source": [
        "!pip install bs4\n",
        "# installation du package Beautiful soup\n",
        "\n",
        "import requests # permet de requeter les informations \n",
        "from bs4 import BeautifulSoup # Parsing - permet de recherché les informations precisément \n",
        "from urllib.request import urlopen\n",
        "import pandas as pd # j'install pandas pour passer ce que je récupère du scrap en tablo, c'est plus joli\n",
        "\n",
        "url=\"https://www.marmiton.org/recettes/recherche.aspx?aqt=desert-no%C3%ABl&start=84&page=8\" # url adresse \n",
        "html = requests.get(url) # requete de la page \n",
        "html # REPONSE 406\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(html.text,'html.parser')\n",
        "# lecture de la page  avec tous les codes - on crée une soupe avec le code HTML\n",
        "soup # affiche le code et ses balises de la page. ICI CELA PERMET D'ANALYSER LES\n",
        "\n",
        "scrap1 = soup.find_all('div', {\"class\" : \"recipe-card\"})\n",
        "# Grace à l'inspecteur d'éléments, on trouve la balise permettant d'identifier le cadre DES RECETTES\n",
        "# AUTRE SYNTAX scrap1 = soup.find_all(style=\"border-top: 1px solid #251836; padding: 0 0 5px 7px;\")\n",
        "scrap1\n",
        "\n",
        "len(scrap1)\n",
        "\n",
        "#\n",
        "titre = scrap1[0].find('h4', {'class':'recipe-card__title'}) # \n",
        "#titre.text\n",
        "\n",
        "#note = scrap1[0].find('span', {'class':'recipe-card__rating__value'}) # \n",
        "#note \n",
        "\n",
        "#Avis = scrap1[0].find('span', {'class':'mrtn-font-discret'}) #\n",
        "#Avis\n",
        "\n",
        "#Ingrédients = scrap1[0].find('div', {'class':'recipe-card__description'}) # \n",
        "#Ingrédients\n",
        "\n",
        "time = scrap1[0].find('span', {'class':'recipe-card__duration__value'})\n",
        "time\n",
        "\n",
        "urlpages = []\n",
        "\n",
        "for i in range(2, 20):\n",
        "    url = 'https://www.marmiton.org/recettes/recherche.aspx?aqt=desert-no%C3%ABl&page=' + str(i) #autre méthode\n",
        "    urlpages.append(url)\n",
        "urlpages\n",
        "\n",
        "# ==== > VERSION DEFINITIVE\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from random import randint\n",
        "import re\n",
        "import sys\n",
        "\n",
        "# functions\n",
        "\n",
        "def m(exp, txt):\n",
        "  p = re.compile(exp)\n",
        "  try:\n",
        "    return re.search(p, txt).group(1)\n",
        "  except:\n",
        "    return \"not found\"   \n",
        "\n",
        "def goodTime(time):\n",
        "    originalOne = time\n",
        "    time = re.sub(r\"\\s+\", \"\", time)\n",
        "    total = 0\n",
        "\n",
        "    # for hours\n",
        "    hour = m(r\"(\\d+)h\", time)\n",
        "    time = re.sub(r\"\\d+h\", \"\", time)\n",
        "    if (hour != \"not found\"):\n",
        "        total+=int(hour)*60\n",
        "\n",
        "    # for minuts\n",
        "    mi = m(r\"(\\d+)min\", time)\n",
        "    time = re.sub(r\"\\d+min\", \"\", time)\n",
        "    if (mi != \"not found\"):\n",
        "        total+=int(mi)\n",
        "\n",
        "    # for days\n",
        "    days = m(r\"(\\d+)j\", time)\n",
        "    time = re.sub(r\"\\d+j\", \"\", time)\n",
        "    if (days != \"not found\"):\n",
        "        total+=int(days)*24*60\n",
        "\n",
        "    # for leaving minuts\n",
        "    if re.search(\"^\\d+$\", time):\n",
        "      total+=int(time)\n",
        "      time = \"\"\n",
        "\n",
        "    return time, originalOne, total\n",
        "\n",
        "def getSoup(url): # soup en boucle \n",
        "  html = requests.get(url, headers={'User-Agent': navigator})\n",
        "  return BeautifulSoup(html.text, 'html.parser')\n",
        "\n",
        "def toClean(s):\n",
        "  ss = re.sub(r\"\\n$\", \"\", s)\n",
        "  ss = re.sub(r\"\\n\", \" \", s)\n",
        "  return ss.strip()\n",
        "\n",
        "def waitSec(maxTimeToWait):\n",
        "  sleep(randint(1,maxTimeToWait))\n",
        "\n",
        "# main\n",
        "\n",
        "maxPage       = 5\n",
        "searchList    = ['Desserts-de-Noël','Entrées-de-Noël','Poissons-de-Noël','Viandes-de-Noël']\n",
        "navigator     = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1)'\n",
        "recipes       = {}\n",
        "tab           = \"   \"\n",
        "rootUrl       = \"https://www.marmiton.org\"\n",
        "\n",
        "# building a dict to contain a selection of recipes\n",
        "\n",
        "print(\"****** Step 1\")\n",
        "for pattern in searchList: \n",
        "    print(\"looking at:\", pattern)\n",
        "    dico_chuck = {}\n",
        "    start = 0\n",
        "    for page in range (1,maxPage+1):\n",
        "        print(tab, \"processing page\", page)\n",
        "        waitSec(3)\n",
        "        url =rootUrl+\"/recettes/recherche.aspx?aqt=\"+pattern+\"&start=\"+str(start)+\"&page=\"+str(page)\n",
        "        print(tab, url)\n",
        "        \n",
        "        soup = getSoup(url)\n",
        "        answers = soup.find_all('div', {\"class\" : \"recipe-card\"}) \n",
        "        print(tab, \"found\", len(answers), \"recipes\")\n",
        "        for answer in answers:\n",
        "\n",
        "            title = answer.find('h4', {'class':'recipe-card__title'}).text\n",
        "\n",
        "            note = answer.find('span', {'class':'recipe-card__rating__value'}).text\n",
        "            note = toClean(note)\n",
        "\n",
        "            nrate = answer.find('span', {'class':'mrtn-font-discret'}).text\n",
        "            nrate = re.sub(r\"sur\", \"\", nrate)\n",
        "            nrate = re.sub(r\"avis\", \"\", nrate)\n",
        "            nrate = re.sub(r\"\\s+\", \"\", nrate)\n",
        "            \n",
        "            ingre = answer.find('div', {'class':'recipe-card__description'}).text\n",
        "            ingre = re.sub(r\"Ingrédients : \", \"\", ingre)\n",
        "            ingre = toClean(ingre)\n",
        "            \n",
        "            time = answer.find('span', {'class':'recipe-card__duration__value'}).text\n",
        "            \n",
        "            recipeUrl  = answer.find('a', {'class':'recipe-card-link'}).get(\"href\") \n",
        "\n",
        "            if title in recipes:\n",
        "              print(tab, \"already known recipe\", title)\n",
        "            else:\n",
        "              print(tab, \"considering recipe\", title)\n",
        "              recipes[title] = {}\n",
        "              recipes[title][\"note\"]      = note\n",
        "              recipes[title][\"nrate\"]     = nrate\n",
        "              recipes[title][\"ingre\"]     = ingre\n",
        "              \n",
        "              (timeLeft, timeOriginal, timeTotal) = goodTime(time)\n",
        "              if (timeLeft == \"\"):\n",
        "                recipes[title][\"time\"] = timeTotal\n",
        "              else:\n",
        "                print(\"unable to convert time, left\", timeLeft, timeOriginal)\n",
        "                sys.exit()\n",
        "\n",
        "              recipes[title][\"rooturl\"]   = url\n",
        "              recipes[title][\"pattern\"]   = pattern\n",
        "              recipes[title][\"recipeurl\"] = rootUrl+recipeUrl\n",
        "\n",
        "        start+=12\n",
        "\n",
        "print(\"total is\", len(recipes), \"recipes found\")\n",
        "\n",
        "print(\"****** Step 2\")\n",
        "\n",
        "for recipe in recipes:\n",
        "    recipeUrl = recipes[recipe][\"recipeurl\"]\n",
        "    print(\"looking at\", recipe, recipeUrl)\n",
        "    soup = getSoup(recipeUrl)\n",
        "    waitSec(3)\n",
        "    div = soup.find('div', {'class':'recipe-infos__budget'})\n",
        "    recipes[recipe][\"budget\"] = div.span.text\n",
        "    div = soup.find('div', {'class':'recipe-infos__level'})\n",
        "    recipes[recipe][\"level\"] = div.span.text\n",
        "    div = soup.find('div', {'class':'recipe-infos-users__notebook'})\n",
        "    \n",
        "    # handling likes and also non-numeric ones eg 1.9k\n",
        "    likes = div.span.text\n",
        "    if re.search(\"\\d+\\.\\dk\", likes) or re.search(\"\\d+k\", likes):\n",
        "        likes = re.sub(r\"\\.\", \"\", likes)\n",
        "        likes = re.sub(r\"k\", \"\", likes)\n",
        "        likes = int(likes) * 100\n",
        "    \n",
        "    if re.search(\"^\\d*$\", str(likes)):\n",
        "        recipes[recipe][\"likes\"] = likes\n",
        "    else:\n",
        "        print(\"non-numeric likes:\", likes)\n",
        "        sys.exit()\n",
        "\n",
        "print(\"bye.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_dict(data=recipes, orient='index')\n",
        "df\n",
        "\n"
      ],
      "metadata": {
        "id": "SvVblz_sGtKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sandbox\n",
        "import sys\n",
        "\n",
        "def m(exp, txt):\n",
        "  p = re.compile(exp)\n",
        "  try:\n",
        "    return re.search(p, txt).group(1)\n",
        "  except:\n",
        "    return \"not found\"   \n",
        "\n",
        "def goodTime(time):\n",
        "    time = re.sub(r\"\\s+\", \"\", time)\n",
        "    total = 0\n",
        "\n",
        "    # for hours\n",
        "    hour = m(r\"(\\d+)h\", time)\n",
        "    time = re.sub(r\"\\d+h\", \"\", time)\n",
        "    if (hour == \"not found\"):\n",
        "        print(\"hour time conversion failed\")\n",
        "        sys.exit()\n",
        "    else:\n",
        "        total+=int(hour)*60\n",
        "\n",
        "    # for minuts\n",
        "    mi = m(r\"(\\d+)min\", time)\n",
        "    time = re.sub(r\"\\d+min\", \"\", time)\n",
        "    if (mi == \"not found\"):\n",
        "        print(\"min time conversion failed\")\n",
        "        sys.exit()\n",
        "    else:\n",
        "        total+=int(mi)\n",
        "\n",
        "    # for days\n",
        "    days = m(r\"(\\d+)j\", time)\n",
        "    time = re.sub(r\"\\d+j\", \"\", time)\n",
        "    if (days == \"not found\"):\n",
        "        print(\"days time conversion failed\")\n",
        "        sys.exit()\n",
        "    else:\n",
        "        total+=int(days)*24*60\n",
        "\n",
        "    return time, total\n",
        "\n",
        "print(goodTime(\"1j 2h10min\"))"
      ],
      "metadata": {
        "id": "J0jq7xpuGxUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SENTIMENTS ANALYSIS"
      ],
      "metadata": {
        "id": "2K3cwt70G3Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests # permet de requeter les informations \n",
        "from bs4 import BeautifulSoup # Parsing - permet de recherché les informations precisément \n",
        "from urllib.request import urlopen\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "csvFile = \"https://raw.githubusercontent.com/cyrilbs/wild-repo/main/marmitton.csv\"\n",
        "df = pd.read_csv(csvFile)\n",
        "df"
      ],
      "metadata": {
        "id": "UOMiCfMZG2ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['ingre'].isnull().values.any()\n",
        "df['ingre'].isnull().sum()"
      ],
      "metadata": {
        "id": "sdbjznGMHCow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['ingre'].isnull().values.any()\n",
        "df['budget'].isnull().sum()"
      ],
      "metadata": {
        "id": "enmPJEkpHDV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['budget'] = df['budget'].fillna('Coût moyen')"
      ],
      "metadata": {
        "id": "2B6B6rEQHG6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Unnamed'].isnull().sum()\n",
        "print(df['Unnamed'])"
      ],
      "metadata": {
        "id": "S8Qa0S1kHHkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "#vectorizer = TfidfVectorizer()\n",
        "\n",
        "X = df[\"ingre\"]\n",
        "y = df[\"budget\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=35, train_size = 0.75)\n",
        "\n",
        "vectorizer.fit(X_train)\n",
        "\n",
        "X_train_CV = vectorizer.transform(X_train)\n",
        "X_test_CV = vectorizer.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(max_iter=150).fit(X_train_CV,y_train)\n",
        "\n",
        "print(model.score(X_train_CV,y_train))\n",
        "print(model.score(X_test_CV,y_test))"
      ],
      "metadata": {
        "id": "Yf0N969mHOAz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}